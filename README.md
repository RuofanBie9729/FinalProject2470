# FinalProject2470
### Introduction
n this project, we intend to perform image segmentation with prostate Magnetic Resonance Imaging(MRI) data.Prostate cancer is the second most frequent cancer diagnosis made in men and the fifth leading causeof death worldwide.[1] A few techniques are used for early detection of prostate cancer, includingblood test, biopsy and imaging tests.  The Magnetic Resonance Imaging (MRI) scans create detailedimages of soft tissues in the body using radio waves and strong magnets. MRI scans can give doctorsa very clear picture of the prostate and nearby areas. [2] MRI of prostate cancer usually consists of twonon-overlapping  adjacent  regions:   peripheralzone (PZ) and the transition zone (TZ). An ex-ample  of  prostate  MRI  with  labeled  zones  is shown  in  Fig. 1.   Identifying  prostate  zones  isimportant  for  diagnostic  and  therapies.   How-ever, the identification work requires substantialexpertise in reading MRI scans. Therefore, auto-matic segmentation of prostate zones is instru-mental for prostate lesion detection.The  problem  of  prostate  zone  segmentationis  challenging  because  of  the  lack  of  a  clearprostate boundary, prostate tissue heterogeneity,and the wide inter-individual variety of prostateshapes.[3] In this project, we will be implementing some existing CNN and RNN models for imagesegmentation using prostate MRI data.   We will use a survey for image segmentation using deeplearning [4] as a guide, implement selected models and compare their performance.

### Related Work
#### Prior Work
There are some work done for segmenting prostate MRI images using deep learning.[5] focus on the work done using fully convolutional neural networks (FCNN). They suggested eightdifferent  FCNNs-based  deep  2D  network  structures  for  automatic  MRI  prostate  segmentation  byanalysing various structures of shortcut connections together with the size of a deep network us-ing the PROMISE12 dataset [6].[7] mentions that 3D neural networks have strong potential for prostate MRI segmentation.  How-ever, substantial computational resources are required due to the large number of parameters. Theyproposed a network architecture calledV-net Light (VnL), which is based on an efficient 3D Modulecalled3D Light,  that minimises the number of network parameters while maintaining state-of-artsegmentation results.  The proposed architecture replaces regular 3D convolutions of the V-net ar-chitecture [8] with novel3D Light modules.  Fig. 2 shows the architecture ofVnL. The V-net modelconsists of encoder and decoder paths with convolutional layers.  To reduce the number of paramers, pooling layers are inserted between the encoder stages.  The novel3D-Light Moduleis used ineach stage of the encoder and decoder. The3D-Light Moduleis a parameter-efficient 3D convolutionalblock consists of parallel convolution blocks, blocks composed of regular convolutions, followed by agroup convolution. It reduces the number of parameters by88%−92%in comparison to V-net. TheVnLachieves comparable results to V-Net on the PROMISE12 dataset [6] while requiring90%lesslearning parameters,90%less hard-disk storage and just3.3%of the FLOPs.[9] proposed a transfer learning method based on deep neural network for prostate MRI segmenta-tion.  They also designed a multi-level edge attention module using wavelet decomposition to over-come the difficulty of ambiguous boundary in the task.

#### Public Implementation
Some of the model architectures we would liketo implement have publicly available implemen-tation.

### Data
We use a set of prostate MRI data from The Medical Segmentation Decathlon – a biomedical image analysis challenge.  The Decathlon challenge made ten data sets available online.  All data setshave been released with a permissive copyright-license (CC-BY-SA 4.0), thus allowing for data shar-ing, redistribution, and commercial usage. [10]
According to [10], all images were de-identified and reformatted to the Neuroimaging InformaticsTechnology Initiative (NIfTI) format https://nifti.nimh.nih.gov. All images were transposed (withoutresampling)  to  the  most  approximate  right-anterior-superior  coordinate  frame,  ensuring  the  datamatrixx−y−zdirection was consistent.  Lastly, non-quantitative modalities (e.g., MRI) were robustmin-max scaled to the same range.  For each segmentation task, a pixel-level label annotation wasprovided.The Decathlon challenge provides users with training set (images and labels) and test set (imageswithout labels).  In order to evaluate the performance with true labels, we only use the training setprovided and randomly select a third of the data to be our own test set.The prostate data set was acquired at Radboud University Medical Center, Nijmegen Medical Centre,Nijmegen, The Netherlands.  It consists of 48 prostate multiparametric MRI (mpMRI) studies, 32 ofthem have corresponding region-of-interest (ROI) targets (background= 0,T Z= 1andP Z= 2).Each study contains approximately 15 to 20 slices of MRI images, resulting in 602 images in total.We will use 10 studies as test set and the remaining 22 studies as training set.  Fig.3 shows the 20slices from one study.

### Methodology
#### Fully Convolutional Networks (FCNs)
As the name suggests, the fully convolutional networks only uses convolutional layers in the archi-tecture. As figure 4 suggested, the FCN is constructed by very deep convolutional layers with a finalpixelwise prediction layer. [11] proposed using FCNs for image segmentation. By combining a deepcoarse layer for appearance information with a shallow fine layer for fine tuning, the architeture pro-posed can implement significant improvement in segmentation accuracy.  Based on the architectureproposed in [11], we consider using three convolutional layer in our FCN model and add a deconvo-lution layer in the final step to implement pixelwise prediction. To mimic the coarse layer combinedwith finer layer architecture in [11], we set the first convolutional layer with 15 filters with stride 4and kernel size3. The second convolutional layer is set to have 8 filters with stride 3 and kernel size3×3.  The first two layers mimics the deep coarse layer, which grasps the appearance information.The third layer is set to have 4 filters with stride 1 and kernel size 2.  We hope the third layer canmimic the fine-tuning process as described in [11].

#### Encoder-Decoder Based Models
A basic encoder-decoder model to implement image segmentation is to use convolutional layers asencoders and then use deconvolutional or convolution-transpose layers for decoders.  As the archi-tecture of the U-Net ([12]) presented in figure 5 shows, we intend to implement a simplified U-Netmodel as the encoder-decoder model.  For the encoder, we consider using three convolutional lay-ers.   All the three convolutional layers would have 10 filters with kernel size 3 and stride size 2.Then the three convolutional layers are followed by three convolution-transpose layers.  All of theconvolution-transpose layers would have 10 filters with kernel size 3 and stride size 2.

#### Dilated Convolutional Models
Due to the translation invariant property of convolutional layer, the FCN model is reliable in pre-diction the presence and roughly the position of objects in an image.  However, as a trade-off be-tween classification accuracy and localization accuracy, the FCN model might not be able to sketchthe  exact  the  outline  of  the  object.   [13]  proposed  to  add  a  fully  connected  CRF  layer  after  theconvolutional layers as presented in figure 6.   In this project,  we intend to add a fully connectedCRF layer to the FCN model described above.  By comparing the performance of FCN model withthis dilated convolutional model, we intend to explore how much improvement the fully connected CRF  can  bring  to  the  FCN  model.   Since  CRF  is  not  a  regular  tensorflow  layer,  we  intend  to  re-fer  to  code  from \link{http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/} to implement thismodelFigure 6:Architecture example of Deep Convolutional with CRF Networks

#### RNN Based Models

The RNN models are usually used in dealing with sequential data.  [14] proposed a Graph-LSTMmodel, which can apply LSTM layer in image segmentation problem.  The LSTM structure usuallyshare forget gates with neighboring nodes to remember information from neighboring nodes.  How-ever, in images, non-neighboring pixel within the same segment might be informative than neighbor-ing pixels.  Consequently, [14] proposed to pass deep convolutional layer results to a1×1convolu-tional layer to generate a confidence map and then use the confidence map results as initial states forthe LSTM layers. In this project, we intend to pass the FCN output, as described in the FCN model toa1×1convolution layer, and use the results of the confidence map as initial state for 2 LSTM layers, which use the output of FCN layers as input.

Since we only have about 600 images, we firstly use data augmentation to add rotated or flippedimaged in the original training set to increase training sample size and also increase the robustness ofour model.  All the four models are trained with 3 epochs through the whole training set with batchsize 100. The trained models are then applied to test set and compute the accuracy metrics describesas below.

### Metrics
The model performance for image segmentation is measured differently from for classification.  Wewill evaluate the model using a few new metrics. [4]

#### Pixel Accuracy
Pixel accuracy (PA) measures the proportion of correctly classified pixels. For K+ 1 classes, the pixelaccuracy is defined as

\begin{equation}
    PA = \frac{\sum_{i=0}^Kp_{ii}}{\sum_{i=0}^K\sum_{j=0}^Kp_{ij}},
\end{equation}

, where $p_{ij}$ is the number of pixels of classipredicted as belonging to classj.
